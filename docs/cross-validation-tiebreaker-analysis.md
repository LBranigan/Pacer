# Cross-Validation Disagreement Resolution: First-Principles Analysis

**Date:** 2026-02-08
**Status:** Research complete, awaiting implementation decision

## Problem Statement

When Reverb and Parakeet disagree on a word, the current system always trusts Reverb's word text. This creates an unprincipled asymmetry: if Reverb heard the correct word it's trusted, but if Parakeet heard it, it's not.

**Triggering case:** User said "wiggle waggle." Reverb produced "wigglewigle-waggle" (garbled BPE doubling). Parakeet produced "wiggle-waggle." (correct). After hyphen splitting, Reverb's "wigglewigle" was aligned against reference "wiggle" and classified as a substitution error — a false penalty.

The cross-validation similarity was ~0.72 (below the 0.8 fuzzy threshold), edit distance 6 (way above the <=1 near-match threshold), so the status was `disagreed` and Reverb's word was kept per `cross-validator.js:234`:
```javascript
const wordOverride = nearMatch ? { word: _normalizeWord(xvWord.word) } : {};
```

## Proposed Solution: Reference-Aware Tiebreaker

When engines disagree (`crossValidation === 'disagreed'`) and one engine's word matches the reference text, trust the matching engine. This leverages ORF's known-text nature without introducing autocorrect.

- If one engine matches reference -> trust it (student likely said the right word)
- If neither matches reference -> keep Reverb's word (substitution error stands)
- If both match differently (shouldn't happen in practice) -> confirmed correct

---

## Engine Architecture & Bias Comparison

### Reverb (WeNet, CTC + Attention Hybrid)

| Property | Detail |
|---|---|
| **Parameters** | 600M |
| **Architecture** | CTC + 6-layer Transformer attention decoder (rescoring) |
| **Training data** | 200K hours, adult professional speech (earnings calls, podcasts, meetings) |
| **Children's speech** | None in training data |
| **Internal LM strength** | Strong (6-layer transformer = implicit LM during attention rescoring) |
| **Verbatimicity** | v=1.0 (verbatim) and v=0.0 (clean) are separate encoder+decoder weights, not just filler removal |
| **Output format** | Unformatted lowercase, no punctuation |
| **Timestamp artifact** | Single-BPE-token words always ~100ms (WeNet `g_time_stamp_gap_ms=100`) |

**Autocorrect mechanism (triple pressure):**
1. BPE frequency bias: common words = single well-learned tokens, rare words = fragmented multi-token sequences
2. CTC beam pruning: low-probability token sequences get pruned early
3. Attention decoder rescoring: 6-layer transformer acts as conditional LM, reranks N-best CTC candidates toward fluent sequences

**Key failure modes on children's speech:**
- Word merging when BPE tokenization collapses adjacent rare words (the "wigglewigle" case)
- Partial attempts decoded as short real words ("str-" -> "stir", "star")
- Filled pauses at children's pitch (300 Hz) decoded as function words ("um" -> "one", "on", "a")
- CTC blank-collapsing merges disfluent repetitions ("th- th- the" -> "the")

### Parakeet TDT (FastConformer + Token-and-Duration Transducer)

| Property | Detail |
|---|---|
| **Parameters** | 600M |
| **Architecture** | FastConformer encoder + 2-layer LSTM prediction network + joint network with duration head |
| **Training data** | 120K hours (10K human-transcribed + 110K pseudo-labeled YouTube) |
| **Children's speech** | None deliberate in training data |
| **Internal LM strength** | Moderate (2-layer LSTM prediction network, weaker than Reverb's transformer) |
| **Output format** | Built-in punctuation, capitalization, ITN (numbers -> digits) |
| **Confidence scores** | Unreliable (often 1.0) |
| **NVIDIA's own caveat** | "Not recommended for word-for-word or incomplete sentence transcription" |

**Autocorrect mechanism:**
1. LSTM prediction network: conditional LM that predicts likely next tokens given previous context
2. Pseudo-labeled training data: 110K hours generated by other ASR models = self-reinforcing autocorrect loop
3. "Garbage in, fluent out" behavior: almost never produces non-word output; either gets the word right or produces a different real word

**Key failure modes on children's speech:**
- Disfluency deletion: tends to smooth over hesitations and partial attempts
- Fluent wrong continuations: prediction network "runs ahead" of acoustic evidence ("b- beau- byoo-" -> "be able to")
- Transducer errors harder to detect because output always looks plausible

### Summary: Both Engines Autocorrect, Differently

| Dimension | Reverb | Parakeet |
|---|---|---|
| LM strength | Strong (6-layer transformer) | Moderate (2-layer LSTM) |
| LM training signal | 200K hrs human-transcribed | 10K human + 110K pseudo-labeled |
| Autocorrect style | Favors common word sequences | Favors "polished" written-form output |
| Disfluency handling | Preserves (v=1.0 mode) | Tends to delete/smooth over |
| Error character | BPE mangling, short real-word substitutions | Fluent wrong continuations |

Neither engine is a reliable ground truth. Neither was trained on children's speech.

---

## Children's ASR: What the Literature Says

### WER by Age/Grade (Singh et al. 2025, Kid-Whisper 2024)

| Grade | Approx Age | Whisper WER (zero-shot) | Notes |
|---|---|---|---|
| K | 5-6 | 84.6% | Extremely unreliable |
| Grade 3 | 8-9 | ~30-40% | Still very high |
| Grade 5 | 10-11 | 18.7% | Best performers in study |
| Grade 8 | 13-14 | ~20% (estimated) | Middle school target range |
| Grade 10 | 15-16 | 22.4% | Anomalous jump due to single-word utterances |
| Adult | 18+ | ~3% | Baseline |

**Key findings:**
- WER consistently improves with age across all models
- Performance gap: 22 percentage points (3% adult vs 25% child for Whisper zero-shot)
- Fine-tuning on children's speech reduces sensitivity to physiological factors
- **For middle school target population: expect ~10-20% WER** with modern models

Sources:
- [Singh et al. 2025 - Causal Analysis of ASR Errors](https://arxiv.org/html/2502.08587v1)
- [Kid-Whisper 2024 (AAAI/ACM)](https://dl.acm.org/doi/10.5555/3716662.3716669)

### Error Type Distribution on Children's Speech

| Error Type | % of Total Errors | Notes |
|---|---|---|
| Substitutions | 50-70% | Dominant error type; acoustic mismatch drives vowel confusions |
| Deletions | 20-35% | Function words, unstressed syllables, word-final consonants |
| Insertions | 10-20% (clean speech) | Inverts for disfluent speech: insertions become dominant |
| Word merging | Under-reported | Counted as substitution + deletion in standard WER |
| Word splitting | Under-reported | CTC vulnerable when children pause mid-word |

### ASR Language Model Interaction with Disfluent Speech

**CTC models (Reverb/WeNet):**
- Collapse repetitions via blank-token merging ("th- th- the" -> "the")
- Map partial attempts to nearest real-word attractor ("str-" -> "stir", "star", "sure")
- Children's filled pauses at 300 Hz decoded as function words

**Transducer models (Parakeet):**
- More aggressive disfluency correction via prediction network
- "Garbage in, fluent out" — almost never produce non-words
- Hallucinate fluent but wrong continuations under severe disfluency
- Errors harder to detect because output always looks plausible

**Critical finding (Gurugubelli et al. 2023):** CTC-based models "fixed" 30-40% of actual reading mistakes, reporting the correct word when the child said something wrong. This is the LM autocorrect bias in action.

### Correctly-Read Words with Unusual Production

**Slow speech generally helps ASR accuracy** on the specific word (more acoustic frames per phoneme). However, word boundaries around slow words are error-prone.

**Hesitant speech (micro-pauses within word) is the danger zone:**
- CTC: 40-60% correct recognition, 20-30% word splitting risk
- Transducers: 60-75% correct recognition, substitution risk on failure

**Duchateau et al. (2011, KU Leuven):** When children read a word correctly, even with hesitation, ASR recognized it correctly 70-85% of the time. Errors were almost always substitutions with phonetically similar words.

Sources:
- Shivakumar et al. 2020 (Speech Recognition for Disorderly Children)
- Rumberg et al. 2021 (Children's read speech)
- Gurugubelli et al. 2023 (CTC on children's read-aloud)
- [Lea et al. 2024 - ASR Bias Against Disfluent Speech](https://arxiv.org/html/2405.06150v1)
- Duchateau et al. 2011 (KU Leuven ORF-ASR)

---

## ORF Assessment Literature

### No Published Multi-Engine ORF System

**This approach is novel.** No published ORF tool uses multi-engine cross-validation:
- **NAEP 2018** uses single proprietary engine (machine-human correlation 0.96)
- **SERDA** (Dutch) uses single engine — precision only 0.31 for word-level errors (69% false positive rate)
- **CORE** uses single engine + latent variable psychometric model
- **Ghana/West Africa** used Whisper V2 alone (10.3% WER)
- **ROVER** technique exists for multi-engine combination but has not been applied to ORF

Sources:
- [NAEP ORF Scoring](https://nces.ed.gov/nationsreportcard/studies/orf/scoring.aspx)
- [Harmsen et al. 2025 - SERDA Validation](https://pmc.ncbi.nlm.nih.gov/articles/PMC12686063/)
- [Lohr et al. 2024 - West Africa ORF](https://link.springer.com/article/10.1007/s40593-024-00435-9)
- [Fiscus 1997 - ROVER](https://ieeexplore.ieee.org/document/659110/)

### Reference-Aware Scoring Approaches (State of the Art)

| Approach | Description | Results | Relevance |
|---|---|---|---|
| **Post-hoc alignment** (traditional) | ASR transcript -> NW alignment to reference | Inherits ASR errors | What our system does now |
| **Prompted Whisper** (Vidal et al. 2025) | Prepend reference text to decoder, train with miscue tokens | 3.9-5.4% WER vs >19% unprompted | Academic frontier; future upgrade path |
| **Forced alignment + confidence** (Hagen et al. 2023) | Force-align to reference, extract per-word confidence | r=0.45 correlation with correctness | Limited accuracy |
| **Two-pass systems** (Gothi et al. 2024) | Standard ASR + alternate decodings under constraints | Reduced false positives | Conceptually similar to our dual-pass Reverb |
| **Disfluency-aware LMs** (Vidal et al. 2025) | Augment vocabulary with `<omit>`, `<substitute>`, `<insert>` | Explicit miscue modeling | Future enhancement |

Sources:
- [Vidal et al. 2025 - Prompting Whisper](https://arxiv.org/html/2505.23627v1)
- [Hagen et al. 2023 - Oral Reading Accuracy](https://arxiv.org/abs/2306.03444)
- [Gothi et al. 2024 - Two-pass Miscue Detection](https://www.isca-archive.org/interspeech_2024/gothi24_interspeech.html)

### False Positives vs False Negatives in ORF

**For screening** (is this child struggling?): false negatives are worse — literature recommends keeping them below 10%.

**For word-level scoring** (WCPM count): false positives are also harmful — SERDA's 69% false positive rate for word-level errors demonstrates the problem. Deflated WCPM undermines trust and can push students into unnecessary intervention.

The reference tiebreaker addresses word-level false positives directly.

---

## Bayesian Analysis: When Engines Disagree

### Prior Probabilities (from ROVER literature + child WER adjustments)

| Scenario | P(student correct) | Tiebreaker action |
|---|---|---|
| Both engines agree, match reference | 95-99% (adult), 88-95% (child) | Already confirmed; no tiebreaker needed |
| One engine matches ref, other doesn't | 75-85% (typical child), 65-75% (struggling) | **Tiebreaker fires: trust matching engine** |
| Both agree on non-reference word | 8-15% correct (strong substitution signal) | Tiebreaker not invoked (both agree) |
| Neither matches reference, engines disagree | 15-25% (typical), 10-20% (struggling) | Tiebreaker: keep Reverb (neither matches) |

### The Dangerous Case: Autocorrect TO Reference

For the tiebreaker to produce a false negative (hide a real error), ALL of these must hold:
1. Student says wrong word W
2. Engine A transcribes W correctly (gets the wrong word right)
3. Engine B's LM autocorrects W -> reference word R
4. Engine B does NOT see the reference text — its LM must independently produce R

**Estimated frequency:** ~15-20% of disagreements. This requires phonetic similarity between the wrong word and the reference, AND the reference having higher LM probability.

### Net Impact on WCPM Accuracy

For a struggling reader at 85% accuracy on a 100-word passage:

| | Without tiebreaker | With tiebreaker |
|---|---|---|
| True correct words | 85 | 85 |
| ASR false errors (correct -> wrong) | ~5-10 | ~1-3 |
| Masked real errors (wrong -> correct) | 0 | ~1-2 |
| Reported WCPM impact | -5 to -10 (deflated) | -1 to +1 (close to truth) |

The tiebreaker gets closer to ground truth because the base rate of correct words is high.

---

## Bias Assessment for ORF Use Case

### Acceptable Biases

| Bias | Why it's OK for ORF |
|---|---|
| Both engines prefer common words | ORF passages use common words; LM bias aligns with reading material |
| Both engines smooth mild hesitation into clean words | WCPM counts correct words, not fluent words. Hesitant "w...wiggle" -> "wiggle" is correct. |
| Near-match resolution (edit dist <=1) | Phonetic parsing noise between engines, not student error |
| Parakeet deleting filled pauses | "um"/"uh" aren't reference words; deletion doesn't affect correctness |
| BPE preferring single-token common words | Works in our favor since reading passage words are mostly common |
| CTC "fixing" 30-40% of reading mistakes | Reduces false positives; if ASR heard the right word, the production was arguably close enough |

### Dangerous Biases (Need Awareness/Guardrails)

| Bias | Risk | Mitigation |
|---|---|---|
| Parakeet autocorrecting phonetically similar wrong word -> reference | Hides ~15% of real substitution errors | Tiebreaker still right 80-85% of the time; net closer to truth |
| Both engines smoothing severe disfluency into same wrong word | "Confirmed" substitution might be a struggle attempt | Existing struggle/near-miss detection via hesitation patterns and timing |
| Both engines failing on same unusual word | Correlated failure, no correct answer | Proper noun forgiveness + dictionary guard handle names; tiebreaker handles words like "wiggle-waggle" |
| One engine "running ahead" of acoustic evidence | Transducer hallucinating fluent continuations | Cross-validation catches this when engines produce different words |

---

## Recommendation

**The reference tiebreaker is the correct approach from first principles:**

1. **ORF is a known-text assessment.** The reference IS part of the measurement. A teacher listening to the student uses the same reference. Using it to break ties is using available information, not introducing bias.

2. **The base rate favors innocence.** Even struggling middle schoolers get 85%+ of words right. When engines disagree and one matches reference, the prior strongly favors "the student said the right word."

3. **The autocorrect-to-reference risk is bounded.** The cross-validator doesn't see the reference text. It can only autocorrect based on its own LM, which must independently produce the reference word — requiring phonetic similarity AND higher LM probability. This limits the risk to ~15-20% of disagreements.

4. **The alternative is worse.** Always trusting Reverb on disagreement introduces systematic bias toward Reverb's specific failure modes (BPE mangling, transformer LM hallucination).

5. **The error mode is less harmful.** Missing an occasional real error (tiebreaker wrong) costs 1 WCPM. Falsely penalizing a correct word (no tiebreaker) also costs 1 WCPM but additionally erodes trust and feels unfair.

### Implementation Notes

- Apply the tiebreaker during alignment (app.js), not during cross-validation (cross-validator.js)
- Cross-validation remains engine-agnostic
- Preserve diagnostic flag: `_tiebreakerUsed: true, _tiebreakerEngine: 'parakeet'`
- Keep `_xvalWord` on every word (already done) for auditability
- Consider: adjust trust based on child's overall accuracy in the passage (adaptive prior)

---

## Literature Gaps

1. No direct study of CTC vs. transducer on the same disfluent child speech corpus
2. No BPE-specific analysis for children's speech
3. Cross-validation priors estimated from adult ROVER studies + child WER offsets, not measured directly
4. Struggling readers (below 25th percentile ORF) are almost unstudied in ASR literature — they fall between "normal children" and "speech disordered children" research populations
5. Most children's ASR studies use controlled lab settings, not noisy classroom/assessment conditions
6. No published system uses multi-engine cross-validation to distinguish ASR artifacts from genuine reading errors — this appears to be a novel contribution
